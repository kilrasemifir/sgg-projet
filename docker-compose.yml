services:
  mongo:
    image: mongo
    ports:
      - "27017:27017"
    volumes:
      - ./data/mongo:/data/db

  mysql:
    image: mysql:8
    ports:
      - "3306:3306"
    volumes:
      - ./data/mysql:/var/lib/mysql
      - ./data/mysql:/var/lib/mysql-files
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: stocks

  gateway:
    image: nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf

  # Zookeeper joue le role de gestionnaire du cluster Kafka
  zookeeper:
    image: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      # Temps entre chaque ping du cluster
      ZOOKEEPER_TICK_TIME: 2000

    # Kafka joue le role de broker de messages
  broker1:
    image: confluentinc/cp-kafka:7.0.1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      # Adresses du broker
      #    depuis l'exterieur: PLAINTEXT://localhost:9092
      #    depuis le cluster: PLAINTEXT_INTERNAL://broker1:19092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker1:9092,PLAINTEXT_INTERNAL://broker1:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  broker2:
    image: confluentinc/cp-kafka:7.0.1
    ports:
      - "9093:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker2:9092,PLAINTEXT_INTERNAL://broker2:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    ports:
      - "9000:8080"
    restart: always
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker1:9092


  # Microservices des produits
  produits:
    build: ./produits
    ports:
      - "8080:8080"
    environment:
      - MONGODB_URI=mongodb://mongo:27017
      - MONGODB_DATABASE=produits
      - KAFKA_BOOTSTRAP_SERVERS=broker1:9092
      - KAFKA_TOPIC=stock

  # Microservices des stocks
  stocks:
    # Construction depuis dockerfile dans le dossier stocks
    # build: ./stocks
    # Récupération depuis dockerhub
    image: killraoux/sgg-stocks
    ports:
      - "8081:8080"
    environment:
      - MYSQL_HOST=mysql
      - KAFKA_BOOTSTRAP_SERVERS=broker1:9092
      - KAFKA_TOPIC=stock

# Partie BigData pipelines avec spark et jupyter
  jupyter-notebook:
    hostname: jupyter-notebook
    container_name: jupyter-notebook
    image: dimajix/jupyter-spark:latest
    command: notebook
    env_file:
      - docker-compose.env
    environment:
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
    expose:
      - 8888
    ports:
      - 8888:8888

  spark-master:
    hostname: spark-master
    container_name: jupyter-spark-master
    image: dimajix/jupyter-spark:latest
    command: master
    env_file:
      - docker-compose.env
    environment:
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
    expose:
      - 6066
      - 7077
      - 9090
    ports:
      - 6066:6066
      - 7077:7077
      - 9090:9090

  spark-slave-1:
    hostname: spark-slave-1
    container_name: jupyter-spark-slave-1
    image: dimajix/jupyter-spark:latest
    command: slave
    env_file:
      - docker-compose.env
    environment:
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
    expose:
      - 9090
    ports:
      - 9091:9090

  spark-slave-2:
    hostname: spark-slave-2
    container_name: jupyter-spark-slave-2
    image: dimajix/jupyter-spark:latest
    command: slave
    env_file:
      - docker-compose.env
    environment:
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
    expose:
      - 9090
    ports:
      - 9099:9090